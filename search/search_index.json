{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#weclome","title":"Weclome","text":"<p>Welcome to OpenPoints library. OpenPoints is a machine learning codebase for point-based methods for point cloud understanding. The biggest difference between OpenPoints and other libraries is that we focus more on reproducibility and fair benchmarking. </p> <ol> <li> <p>Extensibility: supports many representative networks for point cloud understanding, such as PointNet, DGCNN, DeepGCN, PointNet++, ASSANet, PointMLP, PointNeXt, and Pix4Point. More networks can be built easily based on our framework since OpenPoints support a wide range of basic operations including graph convolutions, linear convolutions, local aggregation modules, self-attention, farthest point sampling, ball query, *e.t.c.</p> </li> <li> <p>Reproducibility: all implemented models are trained on various tasks at least three times. Mean\u00b1std is provided in the PointNeXt paper.  Pretrained models and logs are available.</p> </li> <li> <p>Fair Benchmarking: in PointNeXt, we find a large part of performance gain is due to the training strategies. In OpenPoints, all models are trained with the improved training strategies and all achieve much higher accuracy than the original reported value. </p> </li> <li> <p>Ease of Use: Build model, optimizer, scheduler, loss function,  and data loader easily from cfg. Train and validate different models on various tasks by simply changing the <code>cfg\\*\\*.yaml</code> file. </p> <p><pre><code>model = build_model_from_cfg(cfg.model)\ncriterion = build_criterion_from_cfg(cfg.criterion_args)\n</code></pre>  Here is an example of <code>pointnet.yaml</code> (model configuration for PointNet model):  <pre><code>model:\n  NAME: BaseCls\n  encoder_args:\n    NAME: PointNetEncoder\n    in_channels: 4\n  cls_args:\n    NAME: ClsHead\n    num_classes: 15\n    in_channels: 1024\n    mlps: [512,256]\n    norm_args: \n      norm: 'bn1d'\n</code></pre></p> </li> <li> <p>Online logging: Support wandb for checking your results anytime anywhere. </p> <p></p> </li> </ol>"},{"location":"#install","title":"Install","text":"<p><pre><code>git clone git@github.com:guochengqian/PointNeXt.git\ncd PointNeXt\nsource install.sh\n</code></pre> Note:  </p> <p>1) the <code>install.sh</code> requires CUDA 11.3; if another version of CUDA is used,  <code>install.sh</code> has to be modified accordingly; check your CUDA version by: <code>nvcc --version</code> before using the bash file;</p> <p>2) you might need to read <code>install.sh</code> for a step-by-step installation if the bash file (<code>install.sh</code>) does not work for you by any chance;</p> <p>3) for all experiments, we use wandb for online logging. Run <code>wandb --login</code> only at the first time in a new machine. Set <code>wandn.use_wandb=False</code> to use this function. Read the official wandb documentation if needed.</p>"},{"location":"#general-usage","title":"General Usage","text":"<p>All experiments follow the simple rule to train and test (run in the root directory): </p> <pre><code>CUDA_VISIBLE_DEVICES=$GPUs python examples/$task_folder/main.py --cfg $cfg $kwargs\n</code></pre> <ul> <li>$GPUs is the list of GPUs to use, for most experiments (ScanObjectNN, ModelNet40, S3DIS), we only use 1 A100 (GPUs=0)</li> </ul> <ul> <li>$task_folder is the folder name of the experiment. For example, for s3dis segmentation, $task_folder=s3dis</li> </ul> <ul> <li>$cfg is the path to cfg, for example, s3dis segmentation, $cfg=cfgs/s3dis/pointnext-s.yaml</li> </ul> <ul> <li>$kwargs is used to overwrite the default configs. E.g. overwrite the batch size, just appending <code>batch_size=32</code> or <code>--batch_size 32</code>.  As another example, testing in S3DIS area 5, $kwargs should be <code>mode=test --pretrained_path $pretrained_path</code>. </li> </ul>"},{"location":"#model-profiling-flops-throughputs-and-parameters","title":"Model Profiling (FLOPs, Throughputs, and Parameters)","text":"<pre><code>CUDA_VISIBLE_DEVICES=0 python examples/profile.py --cfg $cfg  batch_size=$bs num_points=$pts timing=True flops=True\n</code></pre> <p>For example, profile PointNeXt-S classification model as mentioned in paper:</p> <ul> <li>$cfg = cfgs/scanobjectnn/pointnext-s.yaml</li> <li>$bs = 128</li> <li>$pts = 1024</li> </ul>"},{"location":"changes/","title":"Change Logs &amp; TODOs","text":""},{"location":"changes/#change-logs","title":"Change Logs","text":"<ul> <li>[2022/08/21] Added ShapeNetPart and ScanNet. Support AMP. </li> <li>[2022/07/09] Initial Realse</li> </ul>"},{"location":"changes/#todo","title":"TODO","text":"<ul> <li> clean segmentation</li> <li> support multi-gpu testing for segmentation tasks</li> </ul>"},{"location":"modelzoo/","title":"Model Zoo (Pretrained Models)","text":"<p>We provide the training logs &amp; pretrained models in column <code>our released</code> trained with the improved training strategies proposed by our PointNeXt through Google Drive. </p> <p>TP: Throughput (instance per second) measured using an NVIDIA Tesla V100 32GB GPU and a 32 core Intel Xeon @ 2.80GHz CPU.</p>"},{"location":"modelzoo/#scanobjectnn-hardest-variant-classification","title":"ScanObjectNN (Hardest variant) Classification","text":"<p>Throughput is measured with 128 x 1024 points. </p> name OA/mAcc (Original) OA/mAcc (our released) #params FLOPs Throughput (ins./sec.) PointNet 68.2 / 63.4 75.2 / 71.4 3.5M 1.0G 4212 DGCNN 78.1 / 73.6 86.1 / 84.3 1.8M 4.8G 402 PointMLP 85.4\u00b11.3 / 83.9\u00b11.5 87.7 / 86.4 13.2M 31.4G 191 PointNet++ 77.9 / 75.4 86.2 / 84.4 1.5M 1.7G 1872 PointNeXt-S 87.7\u00b10.4 / 85.8\u00b10.6 88.20 / 86.84 1.4M 1.64G 2040 Pix4Point 87.9 / 86.7 87.9 / 86.7  22.6M 28.0G -"},{"location":"modelzoo/#s3ids-6-fold-segmentation","title":"S3IDS (6-fold) Segmentation","text":"<p>Throughput (TP) is measured with 16 x 15000 points.</p> name mIoU/OA/mAcc (Original) mIoU/OA/mAcc (our released) #params FLOPs TP PointNet++ 54.5 / 81.0 / 67.1 68.1 / 87.6 / 78.4 1.0M 7.2G 186 PointNeXt-S 68.0 / 87.4 / 77.3 68.0 / 87.4 / 77.3 0.8M 3.6G 227 PointNeXt-B 71.5 / 88.8 / 80.2 71.5 / 88.8 / 80.2 3.8M 8.8G 158 PointNeXt-L 73.9 / 89.8 / 82.2 73.9 / 89.8 / 82.2 7.1M 15.2G 115 PointNeXt-XL 74.9 / 90.3 / 83.0 74.9 / 90.3 / 83.0 41.6M 84.8G 46"},{"location":"modelzoo/#s3dis-area-5-segmentation","title":"S3DIS (Area 5) Segmentation","text":"<p>Throughput (TP) is measured with 16 x 15000 points.</p> name mIoU/OA/mAcc (Original) mIoU/OA/mAcc (our released) #params FLOPs TP PointNet++ 53.5 / 83.0 / - 63.6 / 88.3 / 70.2 1.0M 7.2G 186 ASSANet 63.0 / - /- 65.8 / 88.9 / 72.2 2.4M 2.5G 228 ASSANet-L 66.8 / - / - 68.0 / 89.7/ 74.3 115.6M 36.2G 81 PointNeXt-S 63.4\u00b10.8 / 87.9\u00b10.3 / 70.0\u00b10.7 64.2 / 88.2 / 70.7 0.8M 3.6G 227 PointNeXt-B 67.3\u00b10.2 / 89.4\u00b10.1 / 73.7\u00b10.6 67.5 / 89.4 / 73.9 3.8M 8.8G 158 PointNeXt-L 69.0\u00b10.5 / 90.0\u00b10.1 / 75.3\u00b10.8 69.3 / 90.1 / 75.7 7.1M 15.2G 115 PointNeXt-XL 70.5\u00b10.3 / 90.6\u00b10.2 / 76.8\u00b10.7 71.1 / 91.0 / 77.2 41.6M 84.8G 46 Pix4Point 69.6 / 89.9 / 75.2 69.6 / 89.9 / 75.2 23.7M 190G -"},{"location":"modelzoo/#shapenetpart-part-segmentation","title":"ShapeNetpart Part Segmentation","text":"<p>Throughput (TP) is measured with 64*2048 points.</p> name Ins. mIoU / Cat. mIoU (Original) Ins. mIoU / Cat. mIoU (our released) PointNet++ 85.1/81.9 PointNeXt-S 86.7\u00b10.0 / 84.4\u00b10.2 86.7 / 84.2 PointNeXt-S (C=64) 86.9\u00b10.0 / 84.8\u00b10.5 86.9 / 85.2 PointNeXt-S (C=160) 87.0\u00b10.1 / 85.2\u00b10.1 87.1 / 85.4 Pix4Point 86.8 / 85.6 86.8 / 85.6"},{"location":"modelzoo/#modelnet40-classificaiton","title":"ModelNet40 Classificaiton","text":"name OA/mAcc (Original) OA/mAcc (our released) #params FLOPs Throughput (ins./sec.) PointNet++ 91.9 / - 93.0 / 90.7 1.5M 1.7G 1872 PointNeXt-S (C=64) 93.7\u00b10.3 / 90.9\u00b10.5 94.0 / 91.1 4.5M 6.5G 2033"},{"location":"examples/modelnet/","title":"Point cloud classification on ModelNet40","text":"<p>Note in this experiment, we do not use any re-sampled version of ModelNet40 (more than 2K points) or any normal information.  The data we use is: <code>modelnet40_ply_hdf5_2048</code>[1].  </p>"},{"location":"examples/modelnet/#dataset","title":"Dataset","text":"<p>ModelNet40 dataset will be downloaded automatically.</p>"},{"location":"examples/modelnet/#train","title":"Train","text":"<p>For example, train <code>PointNeXt-S</code> <pre><code>CUDA_VISIBLE_DEVICES=0 python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml\n</code></pre></p>"},{"location":"examples/modelnet/#test","title":"Test","text":"<p>test <code>PointNeXt-S (C=64)</code></p> <pre><code>CUDA_VISIBLE_DEVICES=0 python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pointnext-s.yaml model.encoder_args.width=64 mode=test --pretrained_path /path/to/your/pretrained_model\n</code></pre>"},{"location":"examples/modelnet/#reference","title":"Reference","text":"<pre><code>@InProceedings{wu2015modelnet,\n    author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},\n    title = {3D ShapeNets: A Deep Representation for Volumetric Shapes},\n    booktitle = {CVPR},\n    year = {2015}\n}\n</code></pre>"},{"location":"examples/s3dis/","title":"Indoor Point cloud Segmentation on S3DIS","text":"<p>The models are trained on the subsampled point clouds (voxel size = 0.04). The model achieving the best performance on validation is selected to test on the original point clouds (not downsampled). </p>"},{"location":"examples/s3dis/#dataset","title":"Dataset","text":"<p>Please cite the S3DIS paper [1] if you are going to use our presampled datasets.  The presampling is just to collect all point clouds, area by area and room by room, following PointGroup.</p> <pre><code>mkdir -p data/S3DIS/\ncd data/S3DIS\ngdown https://drive.google.com/uc?id=1MX3ZCnwqyRztG1vFRiHkKTz68ZJeHS4Y\ntar -xvf s3disfull.tar\n</code></pre> <p>Organize the dataset as follows:</p> <pre><code>data\n |--- S3DIS\n        |--- s3disfull\n                |--- raw\n                      |--- Area_6_pantry_1.npy\n                      |--- ...\n                |--- processed\n                      |--- s3dis_val_area5_0.040.pkl \n</code></pre>"},{"location":"examples/s3dis/#train","title":"Train","text":"<p>For example, train <code>PointNext-XL</code> <pre><code>CUDA_VISIBLE_DEVICES=0 python examples/segmentation/main.py --cfg cfgs/s3dis/pointnext-xl.yaml\n</code></pre> * change the cfg file to use any other model, e.g. <code>cfgs/s3dis/pointnet++.yaml</code> for training PointNet++ * run the command at the root directory</p>"},{"location":"examples/s3dis/#test-on-area-5","title":"Test on Area 5","text":"<p>Note testing is a must step since evaluation in training is performed only on subsampled point clouds not original point clouds. </p> <p><pre><code>CUDA_VISIBLE_DEVICES=0 bash script/main_segmentation.sh cfgs/s3dis/pointnext-xl.yaml wandb.use_wandb=False mode=test --pretrained_path pretrained/s3dis/pointnext-xl/pointnext-xl-area5/checkpoint/pointnext-xl_ckpt_best.pth\n</code></pre> * add <code>visualize=True</code> to save segmentation results as .obj files</p>"},{"location":"examples/s3dis/#test-on-all-areas","title":"Test on All Areas","text":"<pre><code>CUDA_VISIBLE_DEVICES=0 python examples/segmentation/test_s3dis_6fold.py --cfg cfgs/s3dis/pointnext-xl.yaml mode=test --pretrained_path pretrained/s3dis/pointnext-xl\n</code></pre>"},{"location":"examples/s3dis/#profile-parameters-flops-and-throughput","title":"Profile Parameters, FLOPs, and Throughput","text":"<p><pre><code>CUDA_VISIBLE_DEVICES=0 python examples/profile.py --cfg cfgs/s3dis/pointnext-xl.yaml batch_size=16 num_points=15000 timing=True flops=True\n</code></pre> note:  1. set <code>--cfg</code> to <code>cfgs/s3dis</code> to profile all models under the folder. 2. you have to install the latest version of DeepSpeed from source to get a correct measurement of FLOPs</p>"},{"location":"examples/s3dis/#reference","title":"Reference","text":"<pre><code>@inproceedings{armeni2016s3dis,\n  title={3d semantic parsing of large-scale indoor spaces},\n  author={Armeni, Iro and Sener, Ozan and Zamir, Amir R and Jiang, Helen and Brilakis, Ioannis and Fischer, Martin and Savarese, Silvio},\n  booktitle=CVPR,\n  pages={1534--1543},\n  year={2016}\n}\n</code></pre>"},{"location":"examples/scannet/","title":"Large-Scale 3D Segmentation on ScanNet","text":""},{"location":"examples/scannet/#dataset","title":"Dataset","text":"<p>You can download our preprocessed ScanNet dataset as follows: <pre><code>cd data\ngdown https://drive.google.com/uc?id=1uWlRPLXocqVbJxPvA2vcdQINaZzXf1z_\ntar -xvf ScanNet.tar\n</code></pre> Please cite the ScanNet paper [1] if you are going to conduct experiments on it.</p>"},{"location":"examples/scannet/#train","title":"Train","text":"<p>For example, train <code>PointNext-XL</code> using 8 GPUs by default. <pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python examples/segmentation/main.py --cfg cfgs/scannet/pointnext-xl.yaml </code></pre> * change the cfg file to use any other model, e.g. <code>cfgs/s3dis/pointnet++.yaml</code> for training PointNet++ * run the command at the root directory</p>"},{"location":"examples/scannet/#val","title":"Val","text":"<pre><code>CUDA_VISIBLE_DEVICES=0  python examples/segmentation/main.py --cfg cfgs/scannet/&lt;YOUR_CONFIG&gt; mode=test dataset.test.split=val --pretrained_path &lt;YOUR_CHECKPOINT_PATH&gt;\n</code></pre>"},{"location":"examples/scannet/#test","title":"Test","text":"<p>You can generate Scannet benchmark submission file as follows <pre><code>CUDA_VISIBLE_DEVICES=0 python examples/segmentation/main.py --cfg cfgs/scannet/&lt;YOUR_CONFIG&gt; mode=test dataset.test.split=test no_label=True pretrained_path=&lt;YOUR_CHECKPOINT_PATH&gt;\n</code></pre> Please make sure your checkpoint and your cfg matches with each.</p>"},{"location":"examples/scannet/#reference","title":"Reference","text":"<pre><code>@inproceedings{dai2017scannet,\n    title={{ScanNet}: Richly-annotated {3D} Reconstructions of Indoor Scenes},\n    author={Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\\ss}ner, Matthias},\n    booktitle = CVPR,\n    year = {2017}\n}\n</code></pre>"},{"location":"examples/scanobjectnn/","title":"3D object classification on ScanObjectNN","text":""},{"location":"examples/scanobjectnn/#dataset","title":"Dataset","text":"<p>There are three ways to download the data: </p> <ol> <li> <p>Download from the official website.</p> </li> <li> <p>Or, one can download the data by this command (please submit the  \"ScanObjectNN Terms of Use\" form on their official website before downloading):     <pre><code>mkdir -p data/ScanObjectNN\ncd data/ScanObjectNN\nwget http://hkust-vgd.ust.hk/scanobjectnn/h5_files.zip\n</code></pre></p> </li> <li> <p>Or, one can only download the hardest variant by the following link. Please cite their paper[1] if you use the link to download the data</p> <pre><code>mkdir data\ncd data\ngdown https://drive.google.com/uc?id=1iM3mhMJ_N0x5pytcP831l3ZFwbLmbwzi\ntar -xvf ScanObjectNN.tar\n</code></pre> </li> </ol> <p>Organize the dataset as follows:</p> <pre><code>data\n |--- ScanObjectNN\n            |--- h5_files\n                    |--- main_split\n                            |--- training_objectdataset_augmentedrot_scale75.h5\n                            |--- test_objectdataset_augmentedrot_scale75.h5\n</code></pre>"},{"location":"examples/scanobjectnn/#train","title":"Train","text":"<p>For example, train <code>PointNext-S</code> <pre><code>CUDA_VISIBLE_DEVICES=0 python examples/classification/main.py --cfg cfgs/scanobjectnn/pointnext-s.yaml\n</code></pre></p> <ul> <li>change the cfg file to use any other model, e.g. <code>cfgs/scanobjectnn/pointnet++.yaml</code> for training PointNet++  </li> </ul>"},{"location":"examples/scanobjectnn/#test","title":"Test","text":"<p><pre><code>CUDA_VISIBLE_DEVICES=0 python examples/classification/main.py --cfg cfgs/scanobjectnn/pointnext-s.yaml  mode=test --pretrained_path pretrained/scanobjectnn/pointnext-s/pointnext-s_best.pth </code></pre> * change the cfg file to use any other model, e.g. <code>cfgs/scanobjectnn/pointnet++.yaml</code> for testing PointNet++  </p>"},{"location":"examples/scanobjectnn/#profile-parameters-flops-and-throughput","title":"Profile parameters, FLOPs, and Throughput","text":"<pre><code>CUDA_VISIBLE_DEVICES=0 python examples/profile.py --cfg cfgs/scanobjectnn/pointnext-s.yaml batch_size=128 num_points=1024 timing=True flops=True\n</code></pre> <p>note:  1. set <code>--cfg</code> to <code>cfgs/scanobjectnn</code> to profile all models under the folder.  2. you have to install the latest version of DeepSpeed from source to get a correct measurement of FLOPs</p>"},{"location":"examples/scanobjectnn/#reference","title":"Reference","text":"<pre><code>@inproceedings{uy-scanobjectnn-iccv19,\n      title = {Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data},\n      author = {Mikaela Angelina Uy and Quang-Hieu Pham and Binh-Son Hua and Duc Thanh Nguyen and Sai-Kit Yeung},\n      booktitle = ICCV,\n      year = {2019}\n  }\n</code></pre>"},{"location":"examples/shapenetpart/","title":"Part Segmentation on ShapeNetPart","text":"<p>Note in this experiment, we work on the ShapeNetPart Segmentation. The number of parts for each category is between 2 and 6, with 50 different parts in total.  The data we use is: <code>shapenetcore_partanno_segmentation_benchmark_v0_normal.zip</code>[1]. We uniformly sample 2048 points in training and testing. </p>"},{"location":"examples/shapenetpart/#dataset","title":"Dataset","text":"<p>Download the dataset from the official website, put the data under <code>data/ShapeNetPart/</code>, and then run the training code once. The data will be autmmatically preprocessed (uniformly sample 2048 points). </p> <p>You can also use our preprocessed data provided below: <pre><code>cd data &amp;&amp; mkdir ShapeNetPart &amp;&amp; cd ShapeNetPart\ngdown https://drive.google.com/uc?id=1W3SEE-dY1sxvlECcOwWSDYemwHEUbJIS\ntar -xvf shapenetcore_partanno_segmentation_benchmark_v0_normal.tar\n</code></pre></p> <p>Organize the dataset as follows:</p> <pre><code>data\n |--- ShapeNetPart\n        |--- shapenetcore_partanno_segmentation_benchmark_v0_normal\n                |--- train_test_split\n                      |--- shuffled_train_file_list.json\n                      |--- ...\n                |--- 02691156\n                      |--- 1a04e3eab45ca15dd86060f189eb133.txt\n                      |--- ...               \n                |--- 02773838\n                |--- synsetoffset2category.txt\n                |--- processed\n                        |--- trainval_2048_fps.pkl\n                        |--- test_2048_fps.pkl\n</code></pre>"},{"location":"examples/shapenetpart/#train","title":"Train","text":"<p>For example, train <code>PointNeXt-S</code> using 4 GPUs by default <pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 python examples/shapenetpart/main.py --cfg cfgs/shapenetpart/pointnext-s.yaml\n</code></pre> - change <code>cfg</code> to <code>cfgs/shapenetpart/pointnext-s_c160.yaml</code> to train the best model we report in our paper.  </p>"},{"location":"examples/shapenetpart/#test","title":"Test","text":"<pre><code>CUDA_VISIBLE_DEVICES=0 python examples/shapenetpart/main.py --cfg cfgs/shapenetpart/pointnext-s.yaml mode=test --pretrained_path /path/to/your/pretrained_model\n</code></pre>"},{"location":"examples/shapenetpart/#profile-parameters-flops-and-throughput","title":"Profile parameters, FLOPs, and Throughput","text":"<pre><code>CUDA_VISIBLE_DEVICES=0 python examples/profile.py --cfg cfgs/shapenetpart/pointnext-s.yaml batch_size=64 num_points=2048 timing=True flops=True\n</code></pre>"},{"location":"examples/shapenetpart/#reference","title":"Reference","text":"<pre><code>@article{yi2016shapnetpart,\nAuthor = {Li Yi and Vladimir G. Kim and Duygu Ceylan and I-Chao Shen and Mengyan Yan and Hao Su and Cewu Lu and Qixing Huang and Alla Sheffer and Leonidas Guibas},\nJournal = {SIGGRAPH Asia},\nTitle = {A Scalable Active Framework for Region Annotation in 3D Shape Collections},\nYear = {2016}}`\n</code></pre>"},{"location":"projects/pix4point/","title":"Pix4point","text":""},{"location":"projects/pix4point/#improving-standard-transformer-models-for-3d-point-cloud-understanding-with-image-pretraining","title":"Improving Standard Transformer Models for 3D Point Cloud Understanding with Image Pretraining","text":"<p>by Guocheng Qian, Xingdi Zhang,  Abdullah Hamdi, Bernard Ghanem</p> <p> </p>"},{"location":"projects/pix4point/#arxiv-code","title":"arXiv | code","text":""},{"location":"projects/pix4point/#news","title":"News","text":"<ul> <li> Sep, 2022: code released</li> </ul>"},{"location":"projects/pix4point/#abstract","title":"Abstract","text":"<p>While Standard Transformer (ST) models have achieved impressive success in natural language processing and computer vision, their performance on 3D point clouds is relatively poor. This is mainly due to the limitation of Transformers: a demanding need for large training data. Unfortunately, in the realm of 3D point clouds, the availability of large datasets is a challenge, which exacerbates the issue of training ST models for 3D tasks. In this work, we propose two contributions to improve ST models on point clouds. First, we contribute a new ST-based point cloud network, by using Progressive Point Patch Embedding as the tokenizer and Feature Propagation with global representation appending as the decoder. Our network is shown to be less hungry for data, and enables ST to achieve performance comparable to the state-of-the-art. Second, we formulate a simple yet effective pipeline dubbed \\textit{Pix4Point}, which allows harnessing Transformers pretrained in the image domain to enhance downstream point cloud understanding. This is achieved through a modality-agnostic ST backbone with the help of our proposed tokenizer and decoder specialized in the 3D domain. Pretrained on a large number of widely available images, we observe significant gains of our ST model in the tasks of 3D point cloud classification, part segmentation, and semantic segmentation on ScanObjectNN, ShapeNetPart, and S3DIS benchmarks, respectively. Our code and models are available at PointNeXt repo. </p>"},{"location":"projects/pix4point/#setup-environment","title":"Setup environment","text":"<ul> <li>git clone this repository and install requirements:   <pre><code>git clone git@github.com:guochengqian/Pix4Point.git\ncd Pix4point\nbash install.sh\n</code></pre></li> </ul> <ul> <li>download the ImageNet21k pretrained Transformer, and put it in <code>pretrained/imagenet/small_21k_224.pth</code> <pre><code>gdown https://drive.google.com/file/d/1Iqc-nWVMmm4c8kYshNFcJsthnUy75Jl1/view?usp=sharing --fuzzy\n</code></pre></li> </ul>"},{"location":"projects/pix4point/#imagenet-pretraining","title":"ImageNet Pretraining","text":"<p>Please refer to DeiT's repo for details. </p>"},{"location":"projects/pix4point/#point-cloud-tasks-finetuning","title":"Point Cloud Tasks Finetuning","text":""},{"location":"projects/pix4point/#s3dis","title":"S3DIS","text":"<ul> <li>finetune Image Pretrained Transformer    <pre><code>CUDA_VISIBLE_DEVICES=0 python examples/segmentation/main.py --cfg cfgs/s3dis_sphere_pix4point/pix4point.yaml\n</code></pre></li> </ul> <ul> <li>test   <pre><code>CUDA_VISIBLE_DEVICES=0 python examples/segmentation/main.py --cfg cfgs/s3dis_sphere_pix4point/pix4point.yaml mode=test  pretrained_path=&lt;pretrained_path&gt;\n</code></pre></li> </ul>"},{"location":"projects/pix4point/#scanobjectnn","title":"ScanObjectNN","text":"<ul> <li>finetune    <pre><code>CUDA_VISIBLE_DEVICES=0  python examples/classification/main.py --cfg cfgs/scanobjectnn_pix4point/pvit.yaml\n</code></pre></li> </ul>"},{"location":"projects/pix4point/#modelnet40","title":"ModelNet40","text":"<ul> <li>finetune    <pre><code>CUDA_VISIBLE_DEVICES=0  python examples/classification/main.py --cfg cfgs/modelnet40ply2048/pix4point.yaml\n</code></pre></li> </ul>"},{"location":"projects/pix4point/#shapenetpart","title":"ShapeNetPart","text":"<ul> <li>finetune    <pre><code>CUDA_VISIBLE_DEVICES=0  python examples/shapenetpart/main.py --cfg cfgs/shapenetpart_pix4point/pix4point.yaml </code></pre></li> </ul>"},{"location":"projects/pix4point/#citation","title":"Citation","text":"<p>If you are using our code in your work, please kindly cite the following: <pre><code>@misc{qian2022improving,\n    title={Improving Standard Transformer Models for 3D Point Cloud Understanding with Image Pretraining},\n    author={Guocheng Qian and Xingdi Zhang and Abdullah Hamdi and Bernard Ghanem},\n    year={2022},\n    eprint={2208.12259},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n</code></pre></p>"},{"location":"projects/pointnext/","title":"Pointnext","text":""},{"location":"projects/pointnext/#pointnext-revisiting-pointnet-with-improved-training-and-scaling-strategies","title":"PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies","text":"<p>by Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, Bernard Ghanem</p> <p> </p>"},{"location":"projects/pointnext/#arxiv-openpoints-library","title":"arXiv | OpenPoints Library","text":""},{"location":"projects/pointnext/#news","title":"News","text":"<ul> <li> Sep, 2022: PointNeXt accepted by NeurIPS'22</li> </ul>"},{"location":"projects/pointnext/#abstract","title":"Abstract","text":"<p>PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9\\% to 86.1%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt, the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7\\% on ScanObjectNN, surpassing PointMLP by 2.3\\%2.3\\%, while being 10 \\times10 \\times faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with 74.9\\%74.9\\% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer.</p>"},{"location":"projects/pointnext/#visualization","title":"Visualization","text":"<p>More examples are available in the paper.</p> <p> </p>"},{"location":"projects/pointnext/#citation","title":"Citation","text":"<p>If you find PointNeXt or the OpenPoints codebase useful, please cite: <pre><code>@InProceedings{qian2022pointnext,\n  title   = {PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies},\n  author  = {Qian, Guocheng and Li, Yuchen and Peng, Houwen and Mai, Jinjie and Hammoud, Hasan and Elhoseiny, Mohamed and Ghanem, Bernard},\n  booktitle=Advances in Neural Information Processing Systems (NeurIPS),\n  year    = {2022},\n}\n</code></pre></p>"}]}